# 시스템 패턴 (System Patterns)

## 시스템 아키텍처
본 시스템은 크게 두 가지 주요 시스템으로 구성되며, 이들은 MCP(Model Context Protocol) 서버를 통해 상호작용하거나 외부 AI 시스템과 연동됩니다.

1.  **벡터 DB 기반 문서 검색 MCP 서버**:
    *   **데이터 수집 및 처리**: 기존 문서(요구사항, 설계 등)를 다양한 소스(codebeamer, confluence)에서 수집하여 전처리하고 텍스트 청크로 분할합니다.
    *   **임베딩 및 저장**: 분할된 청크를 벡터 임베딩으로 변환하여 벡터 DB(예: Pinecone)에 저장하고, 관련 메타데이터는 별도의 DB(예: MongoDB)에 저장합니다.
    *   **MCP 서버**: AI 코딩 도구나 다른 AI 시스템으로부터 검색/조회 요청을 받아 벡터 DB와 메타데이터 DB를 활용하여 결과를 제공합니다. 주요 도구로는 `요구사항 검색 도구`와 `요구사항 조회 도구`가 있습니다.

    ```mermaid
    graph TD
        A[기존 문서<br/>(codebeamer, confluence)] --> B(문서 처리 파이프라인<br/>수집, 파싱, 변환)
        B --> C{텍스트 청크 분할}
        C --> D[임베딩 생성기]
        D --> E[(벡터 DB<br/>Pinecone)]
        B -- 메타데이터 --> F[(메타데이터 DB<br/>MongoDB)]
        E --> G[쿼리 엔진]
        F --> G
        G --> H{MCP 서버<br/>(검색/조회 도구)}
        H --> I[AI 시스템 / AI 코딩 도구]
    ```

2.  **AI 기반 요구사항 분석 및 설계 지원 시스템**:
    *   **사용자 입력 처리**: 개발자로부터 새로운 기능에 대한 요구사항(자연어, `prompt.md` 등)을 입력받습니다.
    *   **AI 분석 및 컨텍스트 활용**: 입력된 내용을 분석하고, "벡터 DB 기반 문서 검색 MCP 서버"를 통해 기존 문서(요구사항, 설계, 아키텍처 정보)를 참조하여 컨텍스트를 확보합니다.
    *   **문서 생성**: 분석된 내용과 확보된 컨텍스트를 바탕으로 새로운 요구사항 문서 및 아키텍처 설계 문서를 Markdown 및 Mermaid 다이어그램 형식으로 생성합니다.

    ```mermaid
    graph TD
        A[사용자 입력<br/>(새 기능 요구사항, prompt.md)] --> B{AI 분석 시스템}
        C[벡터 DB 기반<br/>문서 검색 MCP 서버] -- 기존 문서 컨텍스트 --> B
        D[시스템 명세 MCP 서버<br/>(/llms.txt 기반 아키텍처 정보)] -- 아키텍처 컨텍스트 --> B
        B --> E[요구사항 문서 생성<br/>(Markdown)]
        B --> F[아키텍처 설계 문서 생성<br/>(Markdown + Mermaid)]
    ```

3.  **시스템 명세 MCP 서버 (아키텍처 정보 제공)**:
    *   각 프로젝트의 아키텍처 정보는 웹사이트 형태로 제공되며, 표준화된 `/llms.txt` 파일을 포함합니다.
    *   시스템 명세 MCP 서버는 AI 코딩 도구로부터 특정 `project_id`를 받아 해당 프로젝트의 `/llms.txt`를 파싱합니다.
    *   파싱된 정보를 바탕으로 구조화된 아키텍처 정보를 AI 코딩 도구에 제공하여, AI가 프로젝트 컨텍스트에 맞는 지원을 할 수 있도록 합니다.

    ```mermaid
    graph TD
        A[AI 코딩 도구] -- project_id, component_name --> B{시스템 명세 MCP 서버}
        B -- /llms.txt 조회 --> C[아키텍처 정보 웹사이트<br/>(프로젝트별 /llms.txt)]
        C -- /llms.txt 내용 --> B
        B -- 구조화된 아키텍처 정보 --> A
    ```

## 주요 기술적 결정
-   **문서 중심 개발 (Code as a Document - CaD)**: 개발의 많은 부분을 Markdown과 같은 문서 형식으로 관리하고, 이를 기반으로 AI가 코드 생성 및 기타 작업을 수행하도록 합니다. `prompt.md` -> `design.md` -> `code/` 와 같은 흐름을 지향합니다.
-   **MCP (Model Context Protocol) 활용**: 시스템 간, 그리고 AI 시스템과의 연동을 위해 MCP를 표준 인터페이스로 사용합니다. 이를 통해 각 서버는 특정 "도구(tool)"나 "리소스(resource)"를 제공하는 형태로 구성됩니다.
-   **벡터 데이터베이스 활용**: 기존 문서의 의미론적 검색을 위해 벡터 DB(예: Pinecone)를 핵심 저장소로 사용합니다.
-   **LLM (Large Language Model) 통합**: 요구사항 분석, 설계 문서 생성, 코드 생성 등 다양한 작업에 LLM(예: OpenAI API, 로컬 LLM)을 활용합니다.
-   **프로젝트별 컨텍스트 분리**: 모든 데이터(벡터 DB, 메타데이터 DB, 아키텍처 정보) 및 API 요청/응답에 `project_id`를 명시적으로 사용하여 다중 프로젝트 환경에서 컨텍스트를 명확히 구분합니다.
-   **표준화된 아키텍처 정보 접근**: `/llms.txt` 표준을 도입하여 AI가 프로젝트별 시스템 아키텍처 정보를 일관된 방식으로 파악하고 활용할 수 있도록 합니다.

## 사용 중인 디자인 패턴
-   **MCP 서버 패턴**: 각 주요 기능(문서 검색, 아키텍처 정보 제공 등)을 독립적인 MCP 서버로 구성하여 모듈화 및 확장성을 높입니다.
-   **ETL (Extract, Transform, Load) 패턴**: 다양한 소스에서 문서를 추출하고, 벡터 DB에 저장하기 적합한 형태로 변환 및 적재하는 파이프라인을 구성합니다.
-   **RAG (Retrieval Augmented Generation) 패턴**: LLM이 답변이나 문서를 생성할 때, 먼저 벡터 DB에서 관련 정보를 검색(Retrieval)하고, 이 정보를 컨텍스트로 활용하여 생성(Generation)의 정확성과 관련성을 높입니다.
-   **마이크로서비스 아키텍처 (고려)**: 각 MCP 서버나 주요 기능을 독립적인 마이크로서비스로 배포하여 확장성, 유연성, 장애 격리 등을 확보할 수 있습니다. (현재는 논리적 분리, 향후 물리적 분리 고려)

## 컴포넌트 관계
-   **AI 코딩 도구 (클라이언트)**: 개발자와 상호작용하며, "시스템 명세 MCP 서버"와 "문서 MCP 서버"의 주요 클라이언트 역할을 합니다.
-   **시스템 명세 MCP 서버**: "아키텍처 정보 웹사이트"에서 `/llms.txt`를 통해 정보를 가져와 AI 코딩 도구에 제공합니다.
-   **문서 MCP 서버**: "벡터 데이터베이스" 및 "메타데이터 DB"와 상호작용하여 문서 검색 및 조회 기능을 제공합니다.
-   **벡터 데이터베이스**: "문서 처리 파이프라인"으로부터 임베딩된 데이터를 받아 저장합니다.

## 중요한 구현 경로
1.  **요구사항 문서 처리 및 벡터화 경로**: `codebeamer API -> 요구사항 수집기 -> 파서 -> 변환기 -> 텍스트 청크 분할기 -> 임베딩 생성기 -> 벡터 DB / 메타데이터 DB`
2.  **AI 기반 문서 검색 경로**: `AI 코딩 도구 -> 문서 MCP 서버 (쿼리 엔진) -> 벡터 DB / 메타데이터 DB -> 문서 MCP 서버 -> AI 코딩 도구`
3.  **AI 기반 아키텍처 정보 조회 경로**: `AI 코딩 도구 -> 시스템 명세 MCP 서버 -> 아키텍처 정보 웹사이트 (/llms.txt) -> 시스템 명세 MCP 서버 -> AI 코딩 도구`
4.  **AI 기반 설계 문서 생성 경로**: `개발자 (prompt.md) -> AI 코딩 도구 -> (문서 MCP 서버 & 시스템 명세 MCP 서버 통해 컨텍스트 확보) -> 내부 LLM -> AI 코딩 도구 (design.md) -> 개발자`
